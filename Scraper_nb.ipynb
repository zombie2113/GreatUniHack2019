{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='jb3jO1vg9eF89A', \n",
    "                     client_secret='qPUsSxFvYYP6f5_72-5TNy1GaU0', \n",
    "                     user_agent='my_user_agent',\n",
    "                     password='testusername222',\n",
    "                     username='testusername222')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_subreddits = [ 'memeeconomy', 'bikinibottomtwitter', \n",
    " 'trippinthroughtime', 'anime_irl', \n",
    " 'data_irl', 'blackpeopletwitter', \n",
    " 'whitepeopletwitter', 'boottoobig', \n",
    " 'bonehurtingjuice', 'dankchristianmemes', \n",
    " 'fakehistoryporn', 'historymemes', 'musicmemes', 'kenm', 'meow_irl', 'woof_irl', 'prequelmemes', 'sequelmemes', 'OTmemes', 'youdontsurf', 'starterpacks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD ALL THE POSTS\n",
    "results_dict = dict()\n",
    "for subreddit_name in top_subreddits:   \n",
    "    hot_posts = reddit.subreddit(subreddit_name).hot(limit=1000)\n",
    "    # Store results\n",
    "    results_dict[subreddit_name] = [post for post in hot_posts]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXPORT ALL RESULTS TO LIST\n",
    "results = list()\n",
    "for listt in results_dict.values():\n",
    "    for obj in listt:\n",
    "        results.append(obj.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ALL RESULTS TO CSV\n",
    "import csv\n",
    "csv_columns = list(results[0].keys())\n",
    "csv_columns.append('link_flair_template_id')\n",
    "csv_columns.append('post_hint')\n",
    "csv_columns.append('preview')\n",
    "csv_columns.append('media_metadata')\n",
    "csv_columns.append('crosspost_parent_list')\n",
    "\n",
    "csv_columns.append('crosspost_parent')\n",
    "csv_columns.append('author_cakeday')\n",
    "csv_file = \"export.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in results:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOAD ALL IMAGES\n",
    "\n",
    "# Image list url\n",
    "images_url = [post.url for post in results_dict[subreddit_name]]\n",
    "os.mkdir(subreddit_name)\n",
    "\n",
    "# Iterate over list\n",
    "for i in range(len(images_url)):\n",
    "    try:\n",
    "        wget.download(images_url[i], f'{subreddit_name}/{i}.jpg')\n",
    "    except:\n",
    "        print(f'Url not found :( -> {images_url[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
